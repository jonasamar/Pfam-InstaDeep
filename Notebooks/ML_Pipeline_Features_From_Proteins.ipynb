{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "results_path = \"./ML_Pipeline_Features_From_Proteins_results\"\n",
    "os.makedirs(results_path, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read CSV files into pandas DataFrames\n",
    "df_train  = pd.read_csv(\"../random_split/train.csv\", index_col=0)\n",
    "df_dev  = pd.read_csv(\"../random_split/dev.csv\", index_col=0)\n",
    "df_test  = pd.read_csv(\"../random_split/test.csv\", index_col=0)\n",
    "\n",
    "# Factorize the 'family_accession' column and create a new 'label_numeric' column with numeric labels\n",
    "df_train['label_numeric'] = pd.factorize(df_train['family_accession'], sort=True)[0]\n",
    "df_dev['label_numeric'] = pd.factorize(df_dev['family_accession'], sort=True)[0]\n",
    "df_test['label_numeric'] = pd.factorize(df_test['family_accession'], sort=True)[0]\n",
    "\n",
    "# Convert label columns to integer lists\n",
    "y_train = df_train['label_numeric'].astype(int).tolist()\n",
    "y_dev = df_dev['label_numeric'].astype(int).tolist()\n",
    "y_test = df_test['label_numeric'].astype(int).tolist()\n",
    "\n",
    "# Extract sequence as string lists\n",
    "X_train = df_train['sequence'].astype(str).tolist()\n",
    "X_dev = df_dev['sequence'].astype(str).tolist()\n",
    "X_test = df_test['sequence'].astype(str).tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Feature engineering**\n",
    "\n",
    "Here we will create a bunch of features which will hopefully help us discriminate each sequences into one of the 18000 classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "amino acids (N=25): ['W', 'Y', 'U', 'D', 'E', 'P', 'L', 'A', 'I', 'S', 'C', 'F', 'V', 'G', 'N', 'R', 'B', 'Q', 'O', 'K', 'T', 'Z', 'H', 'X', 'M']\n"
     ]
    }
   ],
   "source": [
    "from joblib import Parallel, delayed\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Extracting the list of all amino acids\n",
    "amino_acids = list(set(''.join(X_train)))\n",
    "print(f'amino acids (N={len(amino_acids)}): {amino_acids}')\n",
    "\n",
    "def convert_sequence_to_feature_vector(sequences, amino_acid_list=amino_acids, num_processes=None):\n",
    "    \"\"\"\n",
    "    Convert a list of protein sequences into a feature matrix.\n",
    "\n",
    "    Parameters:\n",
    "    - sequences (list): List of protein sequences.\n",
    "    - amino_acid_list (list): List of amino acids. Default is amino_acids.\n",
    "    - num_processes (int): Number of processes for parallel processing. Default is None.\n",
    "\n",
    "    Returns:\n",
    "    - pd.DataFrame: DataFrame containing the feature matrix.\n",
    "    \"\"\"\n",
    "    # Create a DataFrame with columns as combinations of amino acids and positions\n",
    "    column_names = [\"Length\"] + [f\"{aa}_frequency\" for aa in amino_acids] + [f\"{aa1}->{aa2}_frequency\" for aa1 in amino_acid_list for aa2 in amino_acid_list]\n",
    "    df_features = pd.DataFrame(index=range(len(sequences)), columns=column_names, dtype=int)\n",
    "\n",
    "    with tqdm(total=len(sequences), desc=\"Converting sequences\", unit=\"sequence\") as pbar:\n",
    "        \n",
    "        def process_sequence(i, sequence):\n",
    "            # length of the sequence\n",
    "            n = len(sequence)\n",
    "            \n",
    "            # Initialize features directly in the DataFrame\n",
    "            df_features.loc[i, \"Length\"] = n\n",
    "            \n",
    "            # Amino acid frequencies\n",
    "            for aa1 in amino_acid_list:\n",
    "                count_aa1 = sequence.count(aa1)\n",
    "                df_features.loc[i, f\"{aa1}_frequency\"] = count_aa1 / n\n",
    "                \n",
    "                # Amino acid pair frequencies\n",
    "                for aa2 in amino_acid_list:\n",
    "                    count_aa1_aa2 = sum(1 for a, b in zip(sequence, sequence[1:]) if a == aa1 and b == aa2)\n",
    "                    df_features.loc[i, f\"{aa1}->{aa2}_frequency\"] = count_aa1_aa2 / (count_aa1 if count_aa1 != 0 else 1)\n",
    "            \n",
    "            pbar.update(1)\n",
    "\n",
    "        with Parallel(n_jobs=num_processes, prefer=\"threads\") as parallel:\n",
    "            parallel(\n",
    "                delayed(process_sequence)(i, sequence)\n",
    "                for i, sequence in enumerate(sequences)\n",
    "            )\n",
    "\n",
    "    return df_features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of physical cores: 4\n"
     ]
    }
   ],
   "source": [
    "num_physical_cores = os.cpu_count()\n",
    "print(f\"Number of physical cores: {num_physical_cores}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating train, dev and test feature dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Importing train dataset\n"
     ]
    }
   ],
   "source": [
    "# Training dataset\n",
    "if os.path.exists(Path(results_path, \"train features.csv\")):\n",
    "    print(\"Importing train dataset\")\n",
    "    X_train_features = pd.read_csv(Path(results_path, \"train features.csv\"))\n",
    "else:\n",
    "    print(\"Creating train dataset\")\n",
    "    X_train_features = convert_sequence_to_feature_vector(X_train, num_processes=2)\n",
    "    print(\"Saving train dataset\")\n",
    "    X_train_features.to_csv(Path(results_path, \"train features.csv\"))\n",
    "\n",
    "# Dev dataset\n",
    "if os.path.exists(Path(results_path, \"dev features.csv\")):\n",
    "    print(\"Importing dev dataset\")\n",
    "    X_dev_features = pd.read_csv(Path(results_path, \"dev features.csv\"))\n",
    "else:\n",
    "    print(\"Creating dev dataset\")\n",
    "    X_dev_features = convert_sequence_to_feature_vector(X_dev, num_processes=2)\n",
    "    print(\"Saving dev dataset\")\n",
    "    X_dev_features.to_csv(Path(results_path, \"dev features.csv\"))\n",
    "\n",
    "# Test dataset\n",
    "if os.path.exists(Path(results_path, \"test features.csv\")):\n",
    "    print(\"Importing test dataset\")\n",
    "    X_test_features = pd.read_csv(Path(results_path, \"test features.csv\"))\n",
    "else:\n",
    "    print(\"Creating test dataset\")\n",
    "    X_test_features = convert_sequence_to_feature_vector(X_test, num_processes=2)\n",
    "    print(\"Saving test dataset\")\n",
    "    X_test_features.to_csv(Path(results_path, \"test features.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1086313, 652)\n",
      "1086313\n",
      "(126079, 652)\n",
      "126079\n",
      "(126101, 652)\n",
      "126101\n"
     ]
    }
   ],
   "source": [
    "# Final shapes\n",
    "print(X_train_features.shape)\n",
    "print(len(y_train))\n",
    "print(X_dev_features.shape)\n",
    "print(len(y_dev))\n",
    "print(X_test_features.shape)\n",
    "print(len(y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Extracting a subset of the training**\n",
    "\n",
    "The dataset being very heavy, we will try to subset the training set to build a model on incrementally larger portion of the training set to see how much of the data is required to achieve optimal performances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes removed (N=515) : [10920, 15732, 2691, 15451, 4913, 16756, 15919, 5011, 6054, 10261, 16125, 4818, 3422, 16651, 8423, 14984, 6817, 15878, 16626, 10873, 15308, 17161, 16652, 16688, 11391, 16645, 16737, 10088, 17357, 10601, 16665, 16684, 16692, 7656, 16625, 11392, 4415, 16697, 1461, 17413, 5437, 2597, 15771, 10710, 6049, 5415, 16611, 8621, 16649, 14727, 15332, 10783, 15501, 7449, 8772, 12102, 5324, 16613, 11559, 5164, 12128, 8501, 8715, 7817, 6139, 5020, 16535, 16658, 7467, 9709, 16765, 16655, 8896, 5137, 10637, 12192, 7236, 10345, 6868, 16702, 11555, 2141, 11298, 3172, 17777, 17042, 10823, 6654, 3131, 8597, 10082, 10320, 12297, 6196, 13437, 16638, 10821, 15053, 6752, 10452, 9708, 12311, 2831, 15026, 1371, 12373, 11601, 10824, 2943, 8927, 811, 2330, 6083, 16762, 3346, 11209, 3614, 10696, 4968, 16639, 8064, 16757, 3846, 4505, 7564, 16653, 1452, 8440, 17412, 6085, 16671, 1633, 861, 16654, 15774, 17055, 16764, 5162, 16596, 16773, 2223, 17578, 5857, 16936, 1904, 15165, 15185, 17331, 4683, 10094, 2328, 10761, 16763, 10842, 8537, 16691, 10633, 13863, 4682, 10738, 16687, 3264, 5323, 11853, 8381, 16686, 10743, 2993, 11314, 14237, 3079, 520, 16772, 15370, 8348, 10596, 6794, 8521, 5442, 10916, 9880, 5533, 16629, 8374, 15635, 4554, 10135, 11076, 2952, 5868, 17250, 11373, 3017, 5116, 2098, 2707, 16217, 4396, 12211, 16607, 7480, 6614, 12232, 16931, 15911, 15895, 15066, 4861, 5413, 1594, 15499, 16690, 2770, 16678, 16667, 16591, 9936, 16058, 10074, 5804, 16682, 7619, 2553, 8480, 16521, 10086, 9879, 3040, 16679, 10672, 4340, 2785, 17336, 5068, 13541, 10156, 6303, 16685, 12285, 4179, 10533, 9907, 16937, 10096, 16566, 2254, 16669, 1436, 6314, 2725, 16689, 10238, 7565, 15588, 16660, 3298, 5780, 16610, 13944, 6002, 7424, 16675, 1253, 12236, 17248, 1602, 7563, 5368, 6996, 16650, 7567, 6825, 16627, 5428, 11305, 12449, 16006, 3164, 3142, 16769, 10310, 10062, 8519, 6557, 6192, 6673, 16917, 2498, 15628, 4962, 5322, 5061, 5854, 1305, 10307, 2266, 10856, 13696, 16693, 3239, 10029, 11556, 9952, 16657, 10335, 16642, 16641, 8350, 15495, 12287, 8450, 2163, 6829, 14751, 16134, 12301, 5347, 3513, 1245, 14511, 3005, 10284, 6241, 16659, 17418, 4372, 16633, 1531, 3849, 3110, 16632, 7430, 4184, 7506, 16663, 16661, 6824, 15600, 8778, 16644, 690, 16755, 9626, 16277, 16609, 16640, 16132, 5587, 5017, 16624, 12476, 10811, 16662, 4731, 16879, 4988, 8282, 16129, 7481, 1698, 11563, 6720, 8598, 16670, 5518, 1720, 6564, 10703, 7355, 16567, 16608, 17359, 6790, 1721, 16634, 1722, 16770, 13656, 15407, 4333, 4386, 16612, 10639, 5850, 16646, 16672, 3348, 16119, 16680, 16664, 4475, 16694, 3657, 1527, 1939, 16151, 17296, 10256, 2934, 16656, 5579, 8392, 16919, 10112, 12355, 16622, 9851, 4414, 5742, 11774, 14677, 4260, 8482, 2866, 5870, 16142, 5064, 1485, 17035, 6470, 15785, 11462, 10338, 8563, 11514, 16030, 15509, 6265, 6370, 16636, 10448, 16648, 16750, 17406, 10640, 8410, 8929, 10493, 7426, 4171, 11951, 14249, 10090, 10055, 15784, 3307, 5734, 16599, 1301, 16630, 16916, 16674, 16519, 16771, 5358, 10673, 1617, 6372, 4174, 2139, 16117, 8422, 10109, 3240, 16283, 5370, 4515, 15752, 6799, 16954, 865, 6761, 5018, 3004, 8577, 16696, 16623, 3047, 17302, 16668, 6609, 14734, 11413, 10262, 6381, 8956, 3343, 2780, 9882, 12212, 10562, 10243, 12203, 11581, 12226, 8120, 2552, 2304, 16149, 10127, 16698, 16681, 6576, 15510, 8353, 3236, 7762, 16767, 1377, 6735, 16683, 2515, 16635, 7504, 16666]\n",
      "X_train_filtered shape : (1085798, 652)\n",
      "y_test_filtered length : 1085798\n",
      "X_dev_filtered shape : (123003, 652)\n",
      "y_dev_filtered length : 123003\n",
      "X_test_filtered shape : (123005, 652)\n",
      "y_test_filtered length : 123005\n",
      "X_train_subset.shape : (54290, 652)\n",
      "y_train_subset.shape : (54290,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from collections import Counter\n",
    "\n",
    "# % of the training set we use\n",
    "subset_size = 5e-2\n",
    "\n",
    "# Find the classes represented only once\n",
    "class_counts = Counter(y_train)\n",
    "classes_to_remove = [cls for cls, count in class_counts.items() if count == 1]\n",
    "print(f'Classes removed (N={len(classes_to_remove)}) : {classes_to_remove}')\n",
    "\n",
    "# Filter out instances with classes represented only once\n",
    "## In training set\n",
    "train_mask = ~np.isin(np.array(y_train), classes_to_remove)\n",
    "X_train_filtered = X_train_features[train_mask]\n",
    "y_train_filtered = np.array(y_train)[train_mask]\n",
    "print(f'X_train_filtered shape : {X_train_filtered.shape}')\n",
    "print(f'y_test_filtered length : {len(y_train_filtered)}')\n",
    "\n",
    "# Subsetting the training set\n",
    "_, X_train_subset, _, y_train_subset = train_test_split(\n",
    "    X_train_filtered, y_train_filtered, test_size=subset_size, stratify=y_train_filtered, random_state=42\n",
    ")\n",
    "print(f'X_train_subset.shape : {X_train_subset.shape}')\n",
    "print(f'y_train_subset.shape : {y_train_subset.shape}')\n",
    "\n",
    "## Filtering out labels not represented in train which remain in dev\n",
    "dev_mask = ~np.isin(np.array(y_dev), list(set(y_dev)-set(y_train_subset)))\n",
    "X_dev_filtered = X_dev_features[dev_mask]\n",
    "y_dev_filtered = np.array(y_dev)[dev_mask]\n",
    "print(f'X_dev_filtered shape : {X_dev_filtered.shape}')\n",
    "print(f'y_dev_filtered length : {len(y_dev_filtered)}')\n",
    "\n",
    "## Filtering out labels not represented in train which remain in dev\n",
    "test_mask = ~np.isin(np.array(y_test), list(set(y_test)-set(y_train_subset)))\n",
    "X_test_filtered = X_test_features[test_mask]\n",
    "y_test_filtered = np.array(y_test)[test_mask]\n",
    "print(f'X_test_filtered shape : {X_test_filtered.shape}')\n",
    "print(f'y_test_filtered length : {len(y_test_filtered)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Preprocessing**\n",
    "\n",
    "Here we simply remove the columns which have no variance (columns which are filled only with 1 or only with 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing pipeline\n",
    "preprocessing = Pipeline(\n",
    "    steps=[\n",
    "        (\"variance\", VarianceThreshold(0.)),\n",
    "        (\"standardize\", StandardScaler())\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Preprocessing of X_train\n",
    "X_train_std = pd.DataFrame(\n",
    "                data=preprocessing.fit_transform(X_train_subset),\n",
    "                index=X_train_subset.index,\n",
    "                columns=preprocessing.get_feature_names_out()\n",
    "            )\n",
    "\n",
    "# Applying the same transformation to the dev dataframe\n",
    "X_dev_std = pd.DataFrame(\n",
    "                data=preprocessing.transform(X_dev_filtered),\n",
    "                index=X_dev_filtered.index,\n",
    "                columns=preprocessing.get_feature_names_out()\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(54290, 470)\n",
      "54290\n",
      "(123003, 470)\n",
      "123003\n"
     ]
    }
   ],
   "source": [
    "# Final shapes\n",
    "print(X_train_std.shape)\n",
    "print(len(y_train_subset))\n",
    "print(X_dev_std.shape)\n",
    "print(len(y_dev_filtered))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Models**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn import clone\n",
    "import pickle\n",
    "\n",
    "# Initializing models\n",
    "logit = LogisticRegression(penalty=None,  solver ='saga', class_weight=\"balanced\", max_iter=int(1e6), random_state=42)\n",
    "logit_lasso = LogisticRegression(penalty='l1',  solver ='saga', class_weight=\"balanced\", max_iter=int(1e6), random_state=42)\n",
    "logit_ridge = LogisticRegression(penalty='l2',  solver ='saga', class_weight=\"balanced\", max_iter=int(1e6), random_state=42)\n",
    "xgbc = XGBClassifier()\n",
    "rfc = RandomForestClassifier()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fitting models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_scores = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_logit = clone(logit).fit(X_train_std, y_train_subset)\n",
    "f1_scores['logit'] = f1_score(y_dev_filtered, model_logit.predict(X_dev_std), average = \"weighted\")\n",
    "print(f'logit weighted F1-score : {f1_scores[\"logit\"]}')\n",
    "# Save the trained model to a file\n",
    "filename = Path(results_path, f'subsample_{subset_size}', 'logit_regression_model.sav')\n",
    "pickle.dump(model_logit, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_logit_lasso = clone(logit_lasso).fit(X_train_std, y_train_subset)\n",
    "f1_scores['logit_lasso'] = f1_score(y_dev_filtered, model_logit_lasso.predict(X_dev_std), average = \"weighted\")\n",
    "print(f'logit_lasso weighted F1-score : {f1_scores[\"logit_lasso\"]}')\n",
    "# Save the trained model to a file\n",
    "filename = Path(results_path, f'subsample_{subset_size}', 'logit_lasso_model.sav')\n",
    "pickle.dump(model_logit_lasso, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_logit_ridge = clone(logit_ridge).fit(X_train_std, y_train_subset)\n",
    "f1_scores['logit_ridge'] = f1_score(y_dev_filtered, model_logit_ridge.predict(X_dev_std), average = \"weighted\")\n",
    "print(f'logit_ridge weighted F1-score : {f1_scores[\"logit_ridge\"]}')\n",
    "# Save the trained model to a file\n",
    "filename = Path(results_path, f'subsample_{subset_size}', 'logit_ridge_model.sav')\n",
    "pickle.dump(model_logit_ridge, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_xgbc = XGBClassifier().fit(X_train_std, y_train_subset)\n",
    "f1_scores['xgbc'] = f1_score(y_dev_filtered, model_xgbc.predict(X_dev_std), average = \"weighted\")\n",
    "print(f'model_xgbc weighted F1-score : {f1_scores[\"xgbc\"]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_rfc = RandomForestClassifier().fit(X_train_std, y_train_subset)\n",
    "f1_scores['rfc'] = f1_score(y_dev_filtered, model_rfc.predict(X_dev_std), average = \"weighted\")\n",
    "print(f'model_rfc weighted F1-score : {f1_scores[\"rfc\"]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f1_scores)\n",
    "pd.DataFrame(f1_scores).to_csv(Path(results_path, f'subsample_{subset_size}', 'f1_scores.csv'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
